{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH7PFERFYxYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c986f5-585a-4324-872e-a3343ed296c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 \"/content/drive/My Drive/DpProject/code/Model.py\""
      ],
      "metadata": {
        "id": "gkDiHhpfZAw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c353ba-2eef-4c56-952f-963efda5d067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (MNB) : 85.29%\n",
            "Recall (MNB) : 0.8529411764705882%\n",
            "Cross Validation Accuracy (MNB): 84.50%\n",
            "Accuracy (RF) : 92.31%\n",
            "Recall (RF) : 0.9230769230769231%\n",
            "Cross Validation Accuracy (RF): 86.92%\n",
            "Accuracy (LR) : 91.74%\n",
            "Recall (LR) : 0.917420814479638%\n",
            "Cross Validation Accuracy (LR): 89.19%\n",
            "Accuracy (DT) : 91.97%\n",
            "Recall (DT) : 0.919683257918552%\n",
            "Cross Validation Accuracy (DT): 83.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/DpProject')"
      ],
      "metadata": {
        "id": "HDdQICi_ZGGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing all necessary libraries\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from statistics import mean\n",
        "from nltk.corpus import wordnet\n",
        "import requests\n",
        "from googlesearch import search\n",
        "import os\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from itertools import combinations\n",
        "from time import time\n",
        "from collections import Counter\n",
        "from statistics import mean\n",
        "import operator\n",
        "from xgboost import XGBClassifier\n",
        "import math\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "metadata": {
        "id": "3UUZv1rRZKL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "K6VpAKX2ZgZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7368f14-a024-4dca-a572-bd9467014da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def fetch_synonyms_thesaurus(term):\n",
        "    synonyms = set()\n",
        "    with requests.Session() as session:\n",
        "        url = f'https://www.thesaurus.com/browse/{term}'\n",
        "        try:\n",
        "            response = session.get(url)\n",
        "            response.raise_for_status()  # Raises HTTPError for bad requests\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            container = soup.find('section', {'class': 'MainContentContainer'})\n",
        "            if container:\n",
        "                row = container.find('div', {'class': 'css-191l5o0-ClassicContentCard'})\n",
        "                if row:\n",
        "                    for x in row.find_all('li'):\n",
        "                        synonyms.add(x.get_text())\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error fetching from Thesaurus.com: {e}\")\n",
        "    return synonyms\n",
        "\n",
        "def fetch_synonyms_wordnet(term):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(term):\n",
        "        synonyms.update(syn.lemma_names())\n",
        "    return synonyms\n",
        "\n",
        "def synonyms(term):\n",
        "    \"\"\"Fetches synonyms of a word from Thesaurus.com and WordNet.\"\"\"\n",
        "    thesaurus_synonyms = fetch_synonyms_thesaurus(term)\n",
        "    wordnet_synonyms = fetch_synonyms_wordnet(term)\n",
        "    return thesaurus_synonyms.union(wordnet_synonyms)\n"
      ],
      "metadata": {
        "id": "iJRFYXp3Z4U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_stop_words():\n",
        "    try:\n",
        "        return stopwords.words('english')\n",
        "    except LookupError:\n",
        "        nltk.download('stopwords')\n",
        "        return stopwords.words('english')\n",
        "\n",
        "def initialize_lemmatizer():\n",
        "    return WordNetLemmatizer()\n",
        "\n",
        "def initialize_tokenizer():\n",
        "    return RegexpTokenizer(r'\\w+')\n",
        "\n",
        "stop_words = initialize_stop_words()\n",
        "lemmatizer = initialize_lemmatizer()\n",
        "splitter = initialize_tokenizer()\n",
        "\n"
      ],
      "metadata": {
        "id": "k5C9vOlpaGN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            return pd.read_csv(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file {file_path}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None\n",
        "\n",
        "# Paths to the datasets\n",
        "path_comb = \"/content/drive/My Drive/DpProject/datasets/dis_sym_dataset_comb.csv\"\n",
        "path_norm = \"/content/drive/My Drive/DpProject/datasets/dis_sym_dataset_norm.csv\"\n",
        "\n",
        "df_comb = load_dataset(path_comb)\n",
        "df_norm = load_dataset(path_norm)\n",
        "\n",
        "# Splitting features and labels for the combined dataset\n",
        "if df_comb is not None:\n",
        "    X = df_comb.iloc[:, 1:]\n",
        "    Y = df_comb.iloc[:, 0:1]\n"
      ],
      "metadata": {
        "id": "0YLCwJqwafUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Assuming X, Y are defined as before\n",
        "# Flatten Y for model fitting\n",
        "Y_flattened = Y.values.ravel()\n",
        "\n",
        "# Initialize and fit the model\n",
        "lr = LogisticRegression()\n",
        "try:\n",
        "    lr = lr.fit(X, Y_flattened)\n",
        "    scores = cross_val_score(lr, X, Y_flattened, cv=5)\n",
        "    print(f\"Cross-validation scores: {scores}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error in model fitting or cross-validation: {e}\")\n",
        "\n",
        "# Process for the second dataset\n",
        "X = df_norm.iloc[:, 1:]\n",
        "Y = df_norm.iloc[:, 0:1]\n",
        "dataset_symptoms = list(X.columns)\n",
        "\n",
        "# Function to preprocess user input symptoms\n",
        "def preprocess_symptoms(user_symptoms):\n",
        "    processed_symptoms = []\n",
        "    for sym in user_symptoms:\n",
        "        sym = sym.strip().replace('-', ' ').replace(\"'\", '')\n",
        "        sym = ' '.join([lemmatizer.lemmatize(word) for word in splitter.tokenize(sym)])\n",
        "        processed_symptoms.append(sym)\n",
        "    return processed_symptoms\n",
        "\n",
        "# Take symptoms from user and preprocess them\n",
        "user_input = input(\"Please list your symptoms, separating each one with a comma (,):\\n\")\n",
        "processed_user_symptoms = preprocess_symptoms(user_input.lower().split(','))\n"
      ],
      "metadata": {
        "id": "wQLFqWJPbTt8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a88bdbb7-278b-44b1-8908-0093e7a1d084"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation scores: [0.74872666 0.88455008 0.92190153 0.94227504 0.96208263]\n",
            "Please list your symptoms, separating each one with a comma (,):\n",
            "fever,coughing,tire,headache\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_symptoms(processed_symptoms):\n",
        "    expanded_user_symptoms = []\n",
        "    for user_sym in processed_symptoms:\n",
        "        user_sym_words = user_sym.split()\n",
        "        str_sym = set()\n",
        "\n",
        "        # Generate combinations of words in the symptom\n",
        "        for comb_len in range(1, len(user_sym_words) + 1):\n",
        "            for subset in combinations(user_sym_words, comb_len):\n",
        "                subset_str = ' '.join(subset)\n",
        "                # Fetch synonyms for the subset\n",
        "                subset_synonyms = synonyms(subset_str)\n",
        "                str_sym.update(subset_synonyms)\n",
        "\n",
        "        # Add the original symptom and create the expanded symptom string\n",
        "        str_sym.add(user_sym)\n",
        "        expanded_symptom = ' '.join(str_sym).replace('_', ' ')\n",
        "        expanded_user_symptoms.append(expanded_symptom)\n",
        "\n",
        "    return expanded_user_symptoms\n",
        "\n",
        "user_symptoms = expand_symptoms(processed_user_symptoms)\n",
        "\n",
        "print(\"Expanding query with related medical terms for symptoms entered:\")\n",
        "print(user_symptoms)\n"
      ],
      "metadata": {
        "id": "BpJJvewFh31Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3061a0d5-33a7-44a1-eb62-e184f6a2993b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expanding query with related medical terms for symptoms entered:\n",
            "['fever febricity feverishness pyrexia febrility', 'cough coughing', 'weary sap tyre fag fag out wear down play out outwear bore jade tire out tire wear upon run down fatigue wear exhaust wear out pall', 'worry cephalalgia headache concern head ache vexation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_symptom_sets = [set(sym.split()) for sym in user_symptoms]\n",
        "\n",
        "# Loop over all the symptoms in the dataset\n",
        "found_symptoms = set()\n",
        "for data_sym in dataset_symptoms:\n",
        "    data_sym_set = set(data_sym.split())\n",
        "\n",
        "    # Check similarity with each user symptom\n",
        "    for user_sym_set in user_symptom_sets:\n",
        "        # Count the number of common elements\n",
        "        common_elements_count = len(data_sym_set.intersection(user_sym_set))\n",
        "\n",
        "        # Check if similarity score > 0.5 and add to found_symptoms if true\n",
        "        if common_elements_count / len(data_sym_set) > 0.5:\n",
        "            found_symptoms.add(data_sym)\n",
        "            break\n",
        "\n",
        "# Convert found symptoms from a set to a list\n",
        "found_symptoms = list(found_symptoms)\n"
      ],
      "metadata": {
        "id": "vK6YpZa7jAx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Here are the top matching symptoms based on search query:\")\n",
        "for idx, symp in enumerate(found_symptoms):\n",
        "    print(f\"{idx}: {symp}\")\n",
        "\n",
        "try:\n",
        "    selected_indices = input(\"\\nPlease select the symptoms that apply to you. Enter the numbers corresponding to these symptoms, separated by spaces:\\n\").split()\n",
        "    selected_indices = [int(idx) for idx in selected_indices]\n",
        "    final_symp = [found_symptoms[idx] for idx in selected_indices]\n",
        "except (IndexError, ValueError):\n",
        "    print(\"Invalid input. Please enter valid indices.\")\n",
        "\n",
        "dis_list = set(df_norm[df_norm[final_symp].sum(axis=1) > 0]['label_dis'])\n",
        "counter_dict = {symptom: 0 for symptom in dataset_symptoms if symptom not in final_symp}\n",
        "\n",
        "for dis in dis_list:\n",
        "    for symptom in df_norm.loc[df_norm['label_dis'] == dis, dataset_symptoms].columns[df_norm.loc[df_norm['label_dis'] == dis, dataset_symptoms].any()]:\n",
        "        if symptom in counter_dict:\n",
        "            counter_dict[symptom] += 1\n",
        "\n",
        "print(\"\\nAdditional symptoms based on your selection:\")\n",
        "for symp, count in sorted(counter_dict.items(), key=lambda item: item[1], reverse=True):\n",
        "    if count > 0:\n",
        "        print(f\"{symp} (Co-occurrences: {count})\")\n",
        "\n",
        "counter_list = []\n",
        "for dis in dis_list:\n",
        "    row = df_norm.loc[df_norm['label_dis'] == dis].values.tolist()\n",
        "    if row:\n",
        "        row[0].pop(0)\n",
        "        for idx, val in enumerate(row[0]):\n",
        "            if val != 0 and dataset_symptoms[idx] not in final_symp:\n",
        "                counter_list.append(dataset_symptoms[idx])\n",
        "\n",
        "if counter_list:\n",
        "    dict_symp = dict(Counter(counter_list))\n",
        "    dict_symp_tup = sorted(dict_symp.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "    print(\"Top 10 symptoms co-occurring with the selected ones, sorted by frequency:\")\n",
        "    for symp, count in dict_symp_tup[:10]:  # Slice to get only the top 10\n",
        "        print(f\"{symp}: {count}\")\n",
        "else:\n",
        "    print(\"No additional co-occurring symptoms found.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bNjBRxu8kt1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4e8b38a-300a-4db2-9bfc-22f37af556a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the top matching symptoms based on search query:\n",
            "0: fever\n",
            "1: fatigue\n",
            "2: coughing\n",
            "3: headache\n",
            "\n",
            "Please select the symptoms that apply to you. Enter the numbers corresponding to these symptoms, separated by spaces:\n",
            "0 2 3\n",
            "\n",
            "Additional symptoms based on your selection:\n",
            "testicular pain (Co-occurrences: 17)\n",
            "vomiting (Co-occurrences: 13)\n",
            "barky cough (Co-occurrences: 9)\n",
            "sore throat (Co-occurrences: 9)\n",
            "confusion (Co-occurrences: 8)\n",
            "maculopapular rash (Co-occurrences: 7)\n",
            "diarrhea (Co-occurrences: 6)\n",
            "feeling tired (Co-occurrences: 5)\n",
            "nausea (Co-occurrences: 5)\n",
            "shortness breath (Co-occurrences: 5)\n",
            "swollen lymph node (Co-occurrences: 5)\n",
            "chest pain (Co-occurrences: 4)\n",
            "muscle weakness (Co-occurrences: 4)\n",
            "runny nose (Co-occurrences: 4)\n",
            "unintended weight loss (Co-occurrences: 4)\n",
            "dizziness (Co-occurrences: 3)\n",
            "large lymph node (Co-occurrences: 3)\n",
            "red eye (Co-occurrences: 3)\n",
            "seizure (Co-occurrences: 3)\n",
            "tiredness (Co-occurrences: 3)\n",
            "constipation (Co-occurrences: 2)\n",
            "enlarged lymph node neck (Co-occurrences: 2)\n",
            "eyestrain (Co-occurrences: 2)\n",
            "fatigue (Co-occurrences: 2)\n",
            "joint bone pain (Co-occurrences: 2)\n",
            "low blood pressure (Co-occurrences: 2)\n",
            "lower abdominal pain (Co-occurrences: 2)\n",
            "muscle joint pain (Co-occurrences: 2)\n",
            "non itchy skin ulcer (Co-occurrences: 2)\n",
            "problem vision (Co-occurrences: 2)\n",
            "severe pain (Co-occurrences: 2)\n",
            "skin peeling (Co-occurrences: 2)\n",
            "vaginal bleeding (Co-occurrences: 2)\n",
            "abdominal cramp (Co-occurrences: 1)\n",
            "abscess (Co-occurrences: 1)\n",
            "anemia (Co-occurrences: 1)\n",
            "bad smelling vaginal discharge (Co-occurrences: 1)\n",
            "better sitting worse lying (Co-occurrences: 1)\n",
            "bleeding skin (Co-occurrences: 1)\n",
            "blister sunlight (Co-occurrences: 1)\n",
            "bloody diarrhea (Co-occurrences: 1)\n",
            "blurred vision (Co-occurrences: 1)\n",
            "bruising (Co-occurrences: 1)\n",
            "burning redness eye (Co-occurrences: 1)\n",
            "burning urination (Co-occurrences: 1)\n",
            "change voice (Co-occurrences: 1)\n",
            "characteristic rash (Co-occurrences: 1)\n",
            "chest tightness (Co-occurrences: 1)\n",
            "chill (Co-occurrences: 1)\n",
            "chronic cough (Co-occurrences: 1)\n",
            "close object appear blurry (Co-occurrences: 1)\n",
            "coma (Co-occurrences: 1)\n",
            "cough bloody mucus (Co-occurrences: 1)\n",
            "dark urine (Co-occurrences: 1)\n",
            "depending subtype abdominal pain (Co-occurrences: 1)\n",
            "diarrhea may bloody (Co-occurrences: 1)\n",
            "diarrhea mixed blood (Co-occurrences: 1)\n",
            "diarrhoea (Co-occurrences: 1)\n",
            "difficulty breathing (Co-occurrences: 1)\n",
            "distant object appear blurry (Co-occurrences: 1)\n",
            "distorted blurred vision distance (Co-occurrences: 1)\n",
            "dry cough (Co-occurrences: 1)\n",
            "dry damp skin (Co-occurrences: 1)\n",
            "ear pain (Co-occurrences: 1)\n",
            "enlarged thyroid (Co-occurrences: 1)\n",
            "enlargement tonsil (Co-occurrences: 1)\n",
            "erythema marginatum (Co-occurrences: 1)\n",
            "excessive salivation (Co-occurrences: 1)\n",
            "expanding area redness site tick bite (Co-occurrences: 1)\n",
            "eye strain (Co-occurrences: 1)\n",
            "fear water (Co-occurrences: 1)\n",
            "feeling generally unwell (Co-occurrences: 1)\n",
            "flat discolored spot bump may blister (Co-occurrences: 1)\n",
            "fluid filled blister scab (Co-occurrences: 1)\n",
            "hair loss (Co-occurrences: 1)\n",
            "hallucination (Co-occurrences: 1)\n",
            "hard time reading small print (Co-occurrences: 1)\n",
            "hearing loss (Co-occurrences: 1)\n",
            "high body temperature (Co-occurrences: 1)\n",
            "hoarse voice (Co-occurrences: 1)\n",
            "hold reading material farther away (Co-occurrences: 1)\n",
            "inability child (Co-occurrences: 1)\n",
            "increased breathing rate (Co-occurrences: 1)\n",
            "increased heart rate (Co-occurrences: 1)\n",
            "increased risk infection (Co-occurrences: 1)\n",
            "inflamed eye (Co-occurrences: 1)\n",
            "intellectual disability (Co-occurrences: 1)\n",
            "involuntary muscle movement (Co-occurrences: 1)\n",
            "irregular menstruation (Co-occurrences: 1)\n",
            "irritability (Co-occurrences: 1)\n",
            "itching (Co-occurrences: 1)\n",
            "itchy blister (Co-occurrences: 1)\n",
            "jaundice (Co-occurrences: 1)\n",
            "large lymph node around neck (Co-occurrences: 1)\n",
            "light sensitivity (Co-occurrences: 1)\n",
            "localized breast pain redness (Co-occurrences: 1)\n",
            "loss appetite (Co-occurrences: 1)\n",
            "loss smell (Co-occurrences: 1)\n",
            "low red blood cell (Co-occurrences: 1)\n",
            "memory problem (Co-occurrences: 1)\n",
            "mental change (Co-occurrences: 1)\n",
            "mouth sore (Co-occurrences: 1)\n",
            "mouth ulcer (Co-occurrences: 1)\n",
            "multiple painful joint (Co-occurrences: 1)\n",
            "muscle ache difficulty breathing (Co-occurrences: 1)\n",
            "muscle spasm (Co-occurrences: 1)\n",
            "muscular pain (Co-occurrences: 1)\n",
            "myalgia (Co-occurrences: 1)\n",
            "neck stiffness (Co-occurrences: 1)\n",
            "overlying redness (Co-occurrences: 1)\n",
            "pain sex (Co-occurrences: 1)\n",
            "pain specific bone (Co-occurrences: 1)\n",
            "painful skin (Co-occurrences: 1)\n",
            "painful swelling parotid gland (Co-occurrences: 1)\n",
            "painful swollen joint (Co-occurrences: 1)\n",
            "paralysis (Co-occurrences: 1)\n",
            "photophobia (Co-occurrences: 1)\n",
            "profuse sweating (Co-occurrences: 1)\n",
            "purple colored skin affected area (Co-occurrences: 1)\n",
            "rapid breathing (Co-occurrences: 1)\n",
            "recurring episode wheezing (Co-occurrences: 1)\n",
            "red (Co-occurrences: 1)\n",
            "red rash (Co-occurrences: 1)\n",
            "ringing ear heartbeat (Co-occurrences: 1)\n",
            "sensitivity smell (Co-occurrences: 1)\n",
            "sensitivity sound (Co-occurrences: 1)\n",
            "sharp chest pain (Co-occurrences: 1)\n",
            "skin blister (Co-occurrences: 1)\n",
            "small (Co-occurrences: 1)\n",
            "small blister break open form painful ulcer (Co-occurrences: 1)\n",
            "small blister surrounding swelling (Co-occurrences: 1)\n",
            "sometimes symptom (Co-occurrences: 1)\n",
            "stiff neck (Co-occurrences: 1)\n",
            "sweat (Co-occurrences: 1)\n",
            "swelling abdomen (Co-occurrences: 1)\n",
            "tingling hand foot (Co-occurrences: 1)\n",
            "trouble opening mouth (Co-occurrences: 1)\n",
            "trouble sleeping (Co-occurrences: 1)\n",
            "trouble swallowing (Co-occurrences: 1)\n",
            "vaginal discharge (Co-occurrences: 1)\n",
            "vary depending part brain involved (Co-occurrences: 1)\n",
            "yellow skin (Co-occurrences: 1)\n",
            "Top 10 symptoms co-occurring with the selected ones, sorted by frequency:\n",
            "testicular pain: 17\n",
            "vomiting: 13\n",
            "barky cough: 9\n",
            "sore throat: 9\n",
            "confusion: 8\n",
            "maculopapular rash: 7\n",
            "diarrhea: 6\n",
            "nausea: 5\n",
            "shortness breath: 5\n",
            "feeling tired: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_symp = []\n",
        "found_symptoms = []\n",
        "count = 0\n",
        "\n",
        "for tup in dict_symp_tup:\n",
        "    count += 1\n",
        "    found_symptoms.append(tup[0])\n",
        "\n",
        "    if count % 5 == 0 or count == len(dict_symp_tup):\n",
        "        print(\"\\nCommon co-occurring symptoms:\")\n",
        "        for idx, ele in enumerate(found_symptoms):\n",
        "            print(f\"{idx}: {ele}\")\n",
        "\n",
        "        user_input = input(\"Please review the symptoms: enter the indices of any you have (space-separated), type 'stop' to stop, or '-1' to skip:\\n\").lower().split()\n",
        "\n",
        "        if user_input[0] == 'stop':\n",
        "            break\n",
        "        elif user_input[0] == '-1':\n",
        "            found_symptoms = []\n",
        "            continue\n",
        "\n",
        "        # Validate and add selected symptoms\n",
        "        try:\n",
        "            selected_indices = [int(idx) for idx in user_input if idx.isdigit()]\n",
        "            for idx in selected_indices:\n",
        "                if 0 <= idx < len(found_symptoms):\n",
        "                    final_symp.append(found_symptoms[idx])\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter valid indices.\")\n",
        "\n",
        "        found_symptoms = []\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nwB0YqolDbS",
        "outputId": "b81fff2d-540e-49e6-b687-4354ff4b2c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Common co-occurring symptoms:\n",
            "0: testicular pain\n",
            "1: vomiting\n",
            "2: barky cough\n",
            "3: sore throat\n",
            "4: confusion\n",
            "Please review the symptoms: enter the indices of any you have (space-separated), type 'stop' to stop, or '-1' to skip:\n",
            "3\n",
            "\n",
            "Common co-occurring symptoms:\n",
            "0: maculopapular rash\n",
            "1: diarrhea\n",
            "2: nausea\n",
            "3: shortness breath\n",
            "4: feeling tired\n",
            "Please review the symptoms: enter the indices of any you have (space-separated), type 'stop' to stop, or '-1' to skip:\n",
            "2 4\n",
            "\n",
            "Common co-occurring symptoms:\n",
            "0: swollen lymph node\n",
            "1: chest pain\n",
            "2: muscle weakness\n",
            "3: runny nose\n",
            "4: unintended weight loss\n",
            "Please review the symptoms: enter the indices of any you have (space-separated), type 'stop' to stop, or '-1' to skip:\n",
            "stop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFinalized Symptoms for Prediction Analysis:\")\n",
        "sample_x = [0 for x in range(len(dataset_symptoms))]\n",
        "for val in final_symp:\n",
        "    print(val)\n",
        "    sample_x[dataset_symptoms.index(val)] = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLupD0zdpFrV",
        "outputId": "bd36b065-86d7-48b2-898e-84e380137883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finalized Symptoms for Prediction Analysis:\n",
            "sore throat\n",
            "nausea\n",
            "feeling tired\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def diseaseDetail(term):\n",
        "    query = term + ' wikipedia'\n",
        "    ret = term + \"\\n\"\n",
        "\n",
        "    for sr in search(query, tld=\"co.in\", num=1, stop=1, pause=2):\n",
        "        if 'wikipedia' in sr:\n",
        "            try:\n",
        "                wiki = requests.get(sr)\n",
        "                soup = BeautifulSoup(wiki.content, 'html5lib')\n",
        "                info_table = soup.find(\"table\", {\"class\": \"infobox\"})\n",
        "\n",
        "                if info_table:\n",
        "                    for row in info_table.find_all(\"tr\"):\n",
        "                        header = row.find(\"th\", {\"scope\": \"row\"})\n",
        "                        if header:\n",
        "                            data = row.find(\"td\")\n",
        "                            if data:\n",
        "                                # Remove citations (e.g., [1], [2], etc.)\n",
        "                                data_text = re.sub(r'\\[\\d+\\]', '', ' '.join(data.stripped_strings))\n",
        "                                ret += f\"{header.get_text()} - {data_text}\\n\"\n",
        "                    return ret\n",
        "            except requests.RequestException as e:\n",
        "                return f\"An error occurred: {e}\"\n",
        "\n",
        "    return f\"No detailed information found for {term}.\"\n"
      ],
      "metadata": {
        "id": "oY4FxdWFqsq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Logistic Regression model and make predictions\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X, Y)\n",
        "prediction = lr.predict_proba([sample_x])\n",
        "\n",
        "# Get top k diseases\n",
        "k = 5\n",
        "diseases = list(set(Y['label_dis']))\n",
        "diseases.sort()\n",
        "topk = prediction[0].argsort()[-k:][::-1]\n",
        "\n",
        "print(f\"\\nTop {k} diseases predicted based on symptoms\")\n",
        "topk_dict = {}\n",
        "\n",
        "# Calculate custom probability for each disease\n",
        "for idx, t in enumerate(topk):\n",
        "    match_sym = set()\n",
        "    row = df_norm.loc[df_norm['label_dis'] == diseases[t]].values.tolist()\n",
        "    row[0].pop(0)\n",
        "\n",
        "    for i, val in enumerate(row[0]):\n",
        "        if val != 0:\n",
        "            match_sym.add(dataset_symptoms[i])\n",
        "    prob = (len(match_sym.intersection(set(final_symp))) + 1) / (len(set(final_symp)) + 1)\n",
        "    prob *= mean(scores)\n",
        "    topk_dict[t] = prob\n",
        "\n",
        "# Display top k diseases\n",
        "topk_index_mapping = {}\n",
        "j = 0\n",
        "topk_sorted = dict(sorted(topk_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
        "for key in topk_sorted:\n",
        "    prob = topk_sorted[key] * 100\n",
        "    print(f\"{j} Disease name: {diseases[key]}, Probability: {round(prob, 2)}%\")\n",
        "    topk_index_mapping[j] = key\n",
        "    j += 1\n",
        "\n",
        "try:\n",
        "    select = input(\"\\nMore details about the disease? Enter index of disease or '-1' to discontinue and close the system:\\n\")\n",
        "    if select != '-1':\n",
        "        dis = diseases[topk_index_mapping[int(select)]]\n",
        "        print()\n",
        "        print(diseaseDetail(dis))\n",
        "except ValueError:\n",
        "    print(\"Invalid input. Please enter a valid index or '-1'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANfuy-Fmpk2L",
        "outputId": "64f9f743-6d20-4a4b-b354-712b7b5e5dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 diseases predicted based on symptoms\n",
            "0 Disease name: Rubella, Probability: 66.89%\n",
            "1 Disease name: Influenza, Probability: 66.89%\n",
            "2 Disease name: Hepatitis E, Probability: 44.6%\n",
            "3 Disease name: Hepatitis D, Probability: 44.6%\n",
            "4 Disease name: Fibromyalgia, Probability: 44.6%\n",
            "\n",
            "More details about the disease? Enter index of disease or '-1' to discontinue and close the system:\n",
            "1\n",
            "\n",
            "Influenza\n",
            "Other names - flu, the flu, grippe (French for flu)\n",
            "Specialty - Infectious disease\n",
            "Symptoms - Fever , runny nose , sore throat , muscle pain , headache , coughing , fatigue\n",
            "Usual onset - 14 days after exposure\n",
            "Duration - 28 days\n",
            "Causes - Influenza viruses\n",
            "Prevention - Hand washing , flu vaccines\n",
            "Medication - Antiviral drugs such as oseltamivir\n",
            "Frequency - 35 million severe cases per year  \n",
            "Deaths - >290,000650,000 deaths per year  \n",
            "\n"
          ]
        }
      ]
    }
  ]
}